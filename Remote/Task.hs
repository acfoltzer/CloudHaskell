module Remote.Task where

import qualified Data.Map as Map (Map)
import Data.Dynamic (Dynamic)
import System.FilePath (FilePath)

type ClosureWrapper = Dynamic

data MasterState = MasterState
     {
-- we need to
--   associate each promiseID with a nodeboss who can gives us its data
--   keep a list of nodebosses
--        for each nodeboss
--            which promises are available on that node
--            and associated closures for restarting
--            performance and locality information
--   all known promises
--       and which nodeboss owns them
-- roundrobin node scheduling list
-- if a nodeboss goes down, remove it from lists
-- at regular intervals, rescan available workers
-- if a worker asks promise that can't be redeemed by a nodeboss (due to a restart),
--    exchange it for a new promise (same ID, new nodeboss)
     }

data NodeState
-- Each nodeState needs to know
--    promises available on this system
--    for each local promise
--       where it is now: being generated by a PID; generated in memory; or on a disk
--       # of local restarts; after 3 local restarts, propogate exception to requester
--    when asked for a promise
-- when a generator fails, we can emit a log and restart it
-- if a worker asks for a promise that we don't know about, forward it to master

data WorkerState a = PromiseInMemory a
                   | PromiseOnDisk FilePath

type PromiseDataWrapper = Dynamic


newNodeManager :: NodeId -> TaskM ProcessId
newWorker :: Closure a -> MVar WorkerState -> TaskM ProcessId

data TaskState = TaskState
      {
         tsMaster :: ProcessId,
         tsPromiseCache :: Map.Map PromiseId PromiseDataWrapper
      }

data TaskM a = { runTaskM :: TaskState -> ProcessM (TaskState, a) }

instance Monad TaskM where
   m >>= k = TaskM $ \ts -> do
                (ts',a) <- runTaskM m ts
                (ts'',a') <- runTaskM (k a) (ts')
                return (ts'',a')              
   return x = TaskM $ \ts -> return $ (ts,x)

liftTask :: ProcessM a -> TaskM a
liftTask a = TaskM $ \ts -> a >>= (\x -> return (ts,x))

liftTaskIO :: IO a -> TaskM a
liftTaskIO = liftTask . liftIO

newtype PromiseId = Int

data PromiseList a = PlChunk a (Promise (PromiseList a))
                   | PlNone
type ProiseList2 a = [Promise a]

data Promise a = StandardPromise { psMaster :: ProcessId, psRedeemer :: ProcessId, psId :: PromiseId }
               | StreamPromise

data WStream
data RStream

type RoleId = String

data TaskLocality = forall a.TlNear (Promise a)
                  | TlAt NodeId
                  | TlAutoByNumberOfTasks RoleId
                  | TlAutoByProcessorLoad RoleId

newPromise :: (Serializable a) => Closure (TaskM a) -> TaskM (Promise a)
newPromiseStream :: (Serializable a) => Closure (WStream a -> TaskM ()) -> TaskM (Promise (RStream a))

--the generated task will flush its generated data to disk after a user-configurtable timeout, and reload it if requested; this will save memory

newPromiseWhere :: (Serializable a) => TaskLocality -> Closure (TaskM a) -> TaskM (Promise a)

makePromise :: (Serializable a) => a -> TaskM (Promise a)

readPromise :: Promise a -> TaskM a
{- readpromise sends a message to the nodeboss listed in the Promise, waits for the data.
it also monitors the nodeboss; if it goes down, we contact the master and ask for a replacement,
then repeat -}

startWorkerTask :: ProcessM ()

startMasterTask :: TaskM a -> ProcessM a

streamEmit :: WStream a -> a -> TaskM ()
streamEOF :: WStream a -> TaskM ()
streamRead :: RStream a -> TaskM (Maybe a)


data MapReduceTasks input inter key output 
                    = (Serializable input,
                       Serializable inter,
                       Serializable key,
                       Serializable output) => MapReduceTasks
     {
       mrtMapper :: Closure (Promise [input] -> TaskM [(inter,key)]),
       mrtReducer :: Closure (Promise [(inter,key)] -> TaskM [output]),
       mrtSlicer :: Closure ([input] -> TaskM [(NodeId,[input])]),
       mrtShuffler :: Closure ((inter,key) -> TaskM NodeId)
     }

defaultMapReduceTasks :: MapReduceTasks input inter key output
defaultMapReduceTasks = MapReduceTasks {mrtMapper = undefined,
                                        mrtReducer = undefined,
                                        mrtSlice = defaultMapReduceSlicer,
                                        mrtShuffler = defaultMapReduceShuffler}

mapReduce :: MapReduceTasks input inter key output -> [input] -> TaskM [output]

-- also, there should be a Stream data type that lets files be processed piecemeal to and from promises via files

data Stream a

--also, distributed map

mapRemote :: (Serializable a,Serializable b) => Closure (a -> b) -> [a] -> TaskM [b]


{-
TaskNotes
--------
Calling runTask starts a process (somewhere) which generates the required data and then goes into a receiveWait. The resulting Promise contains (a) the PID of that process (b) the PID of the master node of this network and (c) (probably not) the closure that started it. An attempt to redeem a promise will first check the local node's cache to see if the requested data has already been copied; if so, we're done. Otherwise, send a PromiseRedeem message to the given PID and expect a response containing the data; store the data in the local cache. If the PID does not exist, the node does not exist, or sends a negative response to PromiseRedeem, then we have to ask the master to restart the missing process. This is easy, because the closure to start it is contained in the Promise data structure (alternatively, the closure can be stored by the master). The master will also record that future accesses to the old (missing) PID should be redirected to the new one, so that other tasks attempting to redeem the same promise will be redirected after their first fail, e.g. the broken promise will be exchanged by the master for a new one. (Note that this assumes that two distinct processes will never have the same PID; this is not necessarily the case, so it might be a good idea to add a random unique identifier to node IDs.)
-}

--------------------

module Main where

import Remote.Task
import Remote.Init
import Remote.Process
import Remote.Call

import Control.Monad
import qualified Data.Map as Map

-- contains Vector, Cluster, ClusterId, clusterCenter, sqDistance
import KMeansCommon

$(remoteCall [d|
    kmeansMapper :: Promise [Vector] -> Promise [Cluster] -> TaskM [(Vector,ClusterId)]
    kmeansMapper ppoints pclusters = do points <- readPromise ppoints
                                        clusters <- readPromise pclusters
                                        return $ map (nearby clusters) points
                      where nearby clusters apoint = (apoint,clusterId $ minimumBy (sqDistance apoint) clusters)
    kmeansReducer :: Promise [(Vector,ClusterId)] -> TaskM [Cluster]
    kmeansReducer ppts = do pts <- readPromise ppts
                            let grps = groupBy ((\_,cid1) (_,cid2) -> cid1==cid2) pts
                            return $ map (\grp -> makeCluster (snd grp) $ map fst grp) grps
  |]

initialProcess :: RoleId -> ProcessM ()
initialProcess "TASKWORKER" = startWorkerTask
initialProcess "MASTER" = 
       do points <- liftIO $ liftM read $ readFile "kmeans-points" :: ProcessM [Vector]
          clusters <- liftIO $ liftM read $ readFile "kmeans-clusters" :: ProcessM [Cluster]
          result <- startMasterTask $ 
            do ppoints <- newPromise points
               let loop currentclusters = 
                   newclusters <- mapReduce (defaultMapReduceTask {mrtMapper=kmeansMapper__closure__partial points,mrtReducer=kmeansReducer__closure__partial}) currentclusters -- need some easy way to specify partial applications of closures
                   if newclusters==currentclusters
                      then return newclusters
                      else loop newclusters
               loop clusters
          say $ "Converged! " ++ show result

main = remoteInit (Just "config") [Main.__remoteCallMetaData] initialProcess
